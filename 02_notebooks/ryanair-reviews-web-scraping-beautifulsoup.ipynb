{"cells":[{"cell_type":"markdown","id":"836e1537","metadata":{"papermill":{"duration":0.00503,"end_time":"2024-03-22T14:00:45.012862","exception":false,"start_time":"2024-03-22T14:00:45.007832","status":"completed"},"tags":[],"id":"836e1537"},"source":["# ðŸŒ Ryanair Airline Reviews Web Scraping\n","This notebook showcases web scrapping of the Ryanair airline from the site *AirlineQuality.com* using library `BeautifulSoup`.\n","\n","###  Motivation\n","The purpose of scraping data from the Ryanair airline reviews webpage is to create a dataset containing customer reviews, ratings, and other relevant information. This dataset can be utilized for various purposes such as:\n","\n","#### ðŸ˜Š Sentiment Analysis\n","Analyzing customer sentiments towards Ryanair by studying their reviews and ratings.\n","\n","#### â±ï¸ Performance Evaluation\n","Assessing the performance of Ryanair based on customer feedback regarding aspects like service quality, punctuality, and customer service.\n","\n","#### ðŸ“Š Comparative Analysis\n","Comparing Ryanair's performance with other airlines by analyzing their respective datasets.\n","\n","#### ðŸ¤– Predictive Modeling\n","Building machine learning models to predict customer satisfaction or flight experiences based on review data.\n","\n","#### ðŸ’¡ Business Insights\n","Extracting valuable insights for Ryanair to improve its services, identify areas of improvement, and enhance customer satisfaction.\n","\n"]},{"cell_type":"markdown","id":"9471de07","metadata":{"papermill":{"duration":0.004158,"end_time":"2024-03-22T14:00:45.021883","exception":false,"start_time":"2024-03-22T14:00:45.017725","status":"completed"},"tags":[],"id":"9471de07"},"source":["# Step 1: Importing Necessary Libraries\n","\n","In this step, we import the libraries required for web scraping and parsing HTML content. We'll be using `requests` for fetching webpages and `BeautifulSoup` from bs4 for parsing HTML."]},{"cell_type":"code","execution_count":null,"id":"66155085","metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:00:45.032412Z","iopub.status.busy":"2024-03-22T14:00:45.032009Z","iopub.status.idle":"2024-03-22T14:00:46.165178Z","shell.execute_reply":"2024-03-22T14:00:46.164131Z"},"papermill":{"duration":1.141771,"end_time":"2024-03-22T14:00:46.168023","exception":false,"start_time":"2024-03-22T14:00:45.026252","status":"completed"},"tags":[],"id":"66155085"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd"]},{"cell_type":"code","source":["pip install -r/content/requirements.txt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6EAipqDPFOMO","executionInfo":{"status":"ok","timestamp":1714616883975,"user_tz":180,"elapsed":7741,"user":{"displayName":"Marcela Luciana Tobes","userId":"00439630816236801408"}},"outputId":"affe95d4-a6f9-4f27-fb85-ecea6013b423"},"id":"6EAipqDPFOMO","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting retrying (from -r /content/requirements.txt (line 1))\n","  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->-r /content/requirements.txt (line 1)) (1.16.0)\n","Installing collected packages: retrying\n","Successfully installed retrying-1.3.4\n"]}]},{"cell_type":"code","source":["from retrying import retry\n","import time\n","import traceback"],"metadata":{"id":"Nb65jQxPIYOu"},"id":"Nb65jQxPIYOu","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"0155cd59","metadata":{"papermill":{"duration":0.00426,"end_time":"2024-03-22T14:00:46.177431","exception":false,"start_time":"2024-03-22T14:00:46.173171","status":"completed"},"tags":[],"id":"0155cd59"},"source":[">"]},{"cell_type":"markdown","id":"4f1b50b1","metadata":{"papermill":{"duration":0.004147,"end_time":"2024-03-22T14:00:46.186095","exception":false,"start_time":"2024-03-22T14:00:46.181948","status":"completed"},"tags":[],"id":"4f1b50b1"},"source":["# Step 2: Defining Function to Fetch Webpage Content and Parse HTML\n","\n","Here, we define a function `fetch_and_parse_webpage(url)` that takes a URL as input, fetches the HTML content of the webpage using the `requests.get()` method, and then parses it using Beautiful Soup with the `'html.parser'` parser."]},{"cell_type":"code","execution_count":null,"id":"a2d86e11","metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:00:46.196849Z","iopub.status.busy":"2024-03-22T14:00:46.196283Z","iopub.status.idle":"2024-03-22T14:00:46.203404Z","shell.execute_reply":"2024-03-22T14:00:46.202185Z"},"papermill":{"duration":0.015067,"end_time":"2024-03-22T14:00:46.205657","exception":false,"start_time":"2024-03-22T14:00:46.190590","status":"completed"},"tags":[],"id":"a2d86e11"},"outputs":[],"source":["def fetch_webpage(url):\n","    \"\"\"\n","    Fetches the content of a webpage using requests.\n","\n","    Parameters:\n","        url (str): The URL of the webpage.\n","\n","    Returns:\n","        str: The HTML content of the webpage.\n","    \"\"\"\n","    response = requests.get(url)\n","    if response.status_code == 200:\n","        return response.text\n","    else:\n","        print(f\"Failed to fetch webpage. Status code: {response.status_code}\")\n","        return None\n","\n","def parse_html(html_content):\n","    \"\"\"\n","    Parses HTML content using Beautiful Soup.\n","\n","    Parameters:\n","        html_content (str): The HTML content to parse.\n","\n","    Returns:\n","        BeautifulSoup: A BeautifulSoup object representing the parsed HTML.\n","    \"\"\"\n","    return BeautifulSoup(html_content, 'html.parser')"]},{"cell_type":"markdown","id":"49eefa03","metadata":{"papermill":{"duration":0.004296,"end_time":"2024-03-22T14:00:46.214523","exception":false,"start_time":"2024-03-22T14:00:46.210227","status":"completed"},"tags":[],"id":"49eefa03"},"source":["Now, we check if it works correctly."]},{"cell_type":"code","execution_count":null,"id":"7c3c8b5b","metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:00:46.225396Z","iopub.status.busy":"2024-03-22T14:00:46.224992Z","iopub.status.idle":"2024-03-22T14:00:46.381041Z","shell.execute_reply":"2024-03-22T14:00:46.379829Z"},"papermill":{"duration":0.164474,"end_time":"2024-03-22T14:00:46.383620","exception":false,"start_time":"2024-03-22T14:00:46.219146","status":"completed"},"tags":[],"id":"7c3c8b5b"},"outputs":[],"source":["# Example usage:\n","url = 'https://google.com'\n","html_content = fetch_webpage(url)\n","if html_content:\n","    soup = parse_html(html_content)\n","    # Now you can work with the parsed HTML using Beautiful Soup\n","    # For example, extracting specific elements or information from the webpage"]},{"cell_type":"markdown","id":"d5cded00","metadata":{"papermill":{"duration":0.004272,"end_time":"2024-03-22T14:00:46.392418","exception":false,"start_time":"2024-03-22T14:00:46.388146","status":"completed"},"tags":[],"id":"d5cded00"},"source":["# Step3: Webscraping"]},{"cell_type":"markdown","id":"893a6438","metadata":{"papermill":{"duration":0.004163,"end_time":"2024-03-22T14:00:46.401145","exception":false,"start_time":"2024-03-22T14:00:46.396982","status":"completed"},"tags":[],"id":"893a6438"},"source":["The following code generates a list of URLs for scraping Ryanair airline reviews from the website airlinequality.com. Each URL corresponds to a specific page of reviews, with a maximum of **23 pages**. The `MAX_PAGES` variable determines the maximum number of pages to scrape."]},{"cell_type":"code","execution_count":null,"id":"114f7c56","metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:00:46.411844Z","iopub.status.busy":"2024-03-22T14:00:46.411432Z","iopub.status.idle":"2024-03-22T14:00:46.416355Z","shell.execute_reply":"2024-03-22T14:00:46.415269Z"},"papermill":{"duration":0.012843,"end_time":"2024-03-22T14:00:46.418444","exception":false,"start_time":"2024-03-22T14:00:46.405601","status":"completed"},"tags":[],"id":"114f7c56"},"outputs":[],"source":["MAX_PAGES=23\n","list_url = [f'https://www.airlinequality.com/airline-reviews/ryanair/page/{page}/?sortby=post_date%3ADesc&pagesize=100' for page in range(1, MAX_PAGES+1)]"]},{"cell_type":"markdown","id":"827c0e9b","metadata":{"papermill":{"duration":0.004196,"end_time":"2024-03-22T14:00:46.427334","exception":false,"start_time":"2024-03-22T14:00:46.423138","status":"completed"},"tags":[],"id":"827c0e9b"},"source":["### Initializing DataFrame and Required Variables\n","\n","Here, we initialize an empty DataFrame to store the comments data retrieved from the website. We also define a dictionary `class_to_label `to map class names of HTML elements to corresponding labels for data extraction."]},{"cell_type":"code","execution_count":null,"id":"9d306114","metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:00:46.437901Z","iopub.status.busy":"2024-03-22T14:00:46.437509Z","iopub.status.idle":"2024-03-22T14:00:46.448664Z","shell.execute_reply":"2024-03-22T14:00:46.447478Z"},"papermill":{"duration":0.019122,"end_time":"2024-03-22T14:00:46.450934","exception":false,"start_time":"2024-03-22T14:00:46.431812","status":"completed"},"tags":[],"id":"9d306114"},"outputs":[],"source":["# Initialize an empty DataFrame to store the comments data\n","comments_data = pd.DataFrame(columns=['Date Published', 'Overall Rating', 'Passenger Country', 'Trip_verified', 'Comment title','Comment',\n","                                       'Aircraft', 'Type Of Traveller', 'Seat Type', 'Origin', 'Destination' 'Date Flown',\n","                                       'Seat Comfort', 'Cabin Staff Service', 'Food & Beverages', 'Ground Service',\n","                                       'Value For Money', 'Recommended'])\n","comments_data_list = []\n","\n","class_to_label = {\n","    'aircraft': 'Aircraft',\n","    'type_of_traveller': 'Type Of Traveller',\n","    'cabin_flown': 'Seat Type',\n","    'route': 'Route',\n","    'date_flown': 'Date Flown',\n","    'seat_comfort': 'Seat Comfort',\n","    'cabin_staff_service': 'Cabin Staff Service',\n","    'food_and_beverages': 'Food & Beverages',\n","    'inflight_entertainment':'Inflight Entertainment',\n","    'ground_service': 'Ground Service',\n","    'wifi_and_connectivity':'Wifi & Connectivity',\n","    'value_for_money': 'Value For Money',\n","    'recommended': 'Recommended'\n","}\n"]},{"cell_type":"markdown","id":"306f09b7","metadata":{"papermill":{"duration":0.004237,"end_time":"2024-03-22T14:00:46.459848","exception":false,"start_time":"2024-03-22T14:00:46.455611","status":"completed"},"tags":[],"id":"306f09b7"},"source":["### Scraping Reviews from Multiple Pages\n","\n","This section iterates over each URL in the list of URLs (`list_url`) and scrapes the reviews data from each page. It extracts various information such as date published, overall rating, passenger country, comment title, comment text, and specific ratings related to the flight experience."]},{"cell_type":"code","execution_count":null,"id":"763a4e73","metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:00:46.471098Z","iopub.status.busy":"2024-03-22T14:00:46.470696Z","iopub.status.idle":"2024-03-22T14:01:23.166525Z","shell.execute_reply":"2024-03-22T14:01:23.165253Z"},"papermill":{"duration":36.704826,"end_time":"2024-03-22T14:01:23.169180","exception":false,"start_time":"2024-03-22T14:00:46.464354","status":"completed"},"tags":[],"id":"763a4e73"},"outputs":[],"source":["for url in list_url:\n","    html_content = fetch_webpage(url)\n","\n","    if html_content:\n","        soup = parse_html(html_content)\n","\n","        # Find all comment elements\n","        comments = soup.find_all('article', itemprop='review')  # Only the first 5 comments\n","\n","        for comment in comments:\n","            try:\n","                date_published = comment.find('meta', itemprop='datePublished')['content']\n","                rating = comment.find('span', itemprop='ratingValue')\n","                if rating:\n","                    rating=rating.text\n","                else:\n","                    rating=''\n","\n","                text_header = comment.find('h2', class_='text_header').text\n","\n","                text_sub_header_text = comment.find('h3', class_='text_sub_header userStatusWrapper').get_text(strip=True)\n","                country = text_sub_header_text.split('(')[-1].split(')')[0]\n","\n","                text_content = comment.find('div', class_='text_content', itemprop='reviewBody')\n","\n","                # Find the element containing 'Not Verified' or 'Trip Verified'\n","                verification = text_content.find('strong')\n","                if verification:\n","                    verification =verification.text.strip()\n","                else:\n","                    verification= ''\n","                text_content = text_content.text.strip()\n","\n","                if '|' in text_content:\n","                    text_content= text_content.split('|')[1].strip()\n","\n","\n","                review_ratings = comment.find('table', class_='review-ratings')\n","                review_ratings = comment.find_all('tr')\n","\n","                table_data = {}\n","                for row in review_ratings:\n","                    # Find the header and value cells\n","                    header_cell = row.find('td', class_='review-rating-header')\n","                    value_cell = row.find('td', class_='review-value')\n","                    value2_cell = row.find('td', class_='review-rating-stars')\n","\n","                    # Check if both header and value cells exist\n","                    if header_cell and (value_cell or value2_cell):\n","                        # Get the class name of the header cell\n","                        class_name = header_cell['class'][1]\n","\n","                        # Get the corresponding data label from the class_to_label dictionary\n","                        data_label = class_to_label.get(class_name, '')\n","\n","                        # Store the data label and value in the table_data dictionary\n","                        if value_cell:\n","                            value = value_cell.text.strip()\n","                            # If the feature is 'Route', split the value into origin and destination\n","                            if data_label == 'Route':\n","                                if 'to' in value:\n","                                    origin, destination = value.split(' to ')\n","                                elif '-' in value:\n","                                    origin, destination, _ = value. split('-')\n","                                table_data['Origin'] = origin.strip()\n","                                table_data['Destination'] = destination.strip()\n","                            else:\n","                                table_data[data_label] = value\n","                        else:\n","                            filled_star_spans = value2_cell.find_all('span', class_='star fill')\n","                            table_data[data_label] = int(len(filled_star_spans))\n","\n","                # Append the data from the current comment to the list\n","                comments_data_list.append({'Date Published': date_published, 'Overall Rating': rating,\n","                                           'Passenger Country': country, 'Trip_verified': verification,\n","                                           'Comment title': text_header, 'Comment': text_content, **table_data})\n","\n","            except Exception as e:\n","                print(f'Error en el comentario: {url[60:62]} -> {comments.index(comment)}')\n","                traceback.print_exc()\n"]},{"cell_type":"markdown","id":"ee0b5178","metadata":{"papermill":{"duration":0.004216,"end_time":"2024-03-22T14:01:23.178430","exception":false,"start_time":"2024-03-22T14:01:23.174214","status":"completed"},"tags":[],"id":"ee0b5178"},"source":["### Converting Data into DataFrame and Displaying Results\n","\n","Finally, the scraped data is converted into a pandas DataFrame (`comments_data`). This DataFrame contains all the extracted information from the reviews. We then display the DataFrame to inspect the scraped data."]},{"cell_type":"code","execution_count":null,"id":"293ed3a2","metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:01:23.189310Z","iopub.status.busy":"2024-03-22T14:01:23.188871Z","iopub.status.idle":"2024-03-22T14:01:23.207910Z","shell.execute_reply":"2024-03-22T14:01:23.206919Z"},"papermill":{"duration":0.027464,"end_time":"2024-03-22T14:01:23.210441","exception":false,"start_time":"2024-03-22T14:01:23.182977","status":"completed"},"tags":[],"id":"293ed3a2"},"outputs":[],"source":["# Convert the list of dictionaries into a DataFrame\n","comments_data = pd.DataFrame(comments_data_list)"]},{"cell_type":"markdown","id":"80bbd6d4","metadata":{"papermill":{"duration":0.004759,"end_time":"2024-03-22T14:01:23.219972","exception":false,"start_time":"2024-03-22T14:01:23.215213","status":"completed"},"tags":[],"id":"80bbd6d4"},"source":["# Step 4: Saving the data"]},{"cell_type":"code","execution_count":null,"id":"3e802816","metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:01:23.231656Z","iopub.status.busy":"2024-03-22T14:01:23.231012Z","iopub.status.idle":"2024-03-22T14:01:23.312639Z","shell.execute_reply":"2024-03-22T14:01:23.311779Z"},"papermill":{"duration":0.090286,"end_time":"2024-03-22T14:01:23.315135","exception":false,"start_time":"2024-03-22T14:01:23.224849","status":"completed"},"tags":[],"id":"3e802816"},"outputs":[],"source":["comments_data.to_csv('ryanair_reviews.csv', encoding='utf-8')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4475073,"sourceId":7672164,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":42.93208,"end_time":"2024-03-22T14:01:23.941228","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-03-22T14:00:41.009148","version":"2.4.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}